# tidylearn: A Tidy Approach to Supervised Learning in R

[![CRAN](https://img.shields.io/cran/v/tidylearn)](https://cran.r-project.org/package=tidylearn)
[![R-CMD-check](https://github.com/ces0491/tidylearn/workflows/R-CMD-check/badge.svg)](https://github.com/ces0491/tidylearn/actions)
[![Codecov](https://codecov.io/gh/ces0491/tidylearn/branch/main/graph/badge.svg)](https://app.codecov.io/gh/ces0491/tidylearn)
[![Downloads](https://cranlogs.r-pkg.org/badges/tidylearn)](https://cran.r-project.org/package=tidylearn)

## Overview

`tidylearn` is a comprehensive machine learning package for R that provides a unified, tidy approach to supervised learning models. It is designed to be user-friendly and to integrate seamlessly with the tidyverse ecosystem.

The package simplifies the process of building, evaluating, and comparing different models, while providing rich diagnostics and visualizations to help you understand model behavior and performance.

## Features

- **Unified interface** for various machine learning algorithms
- **Comprehensive model types** including:
  - Linear and polynomial regression
  - Logistic regression and classification
  - Regularization (Ridge, LASSO, Elastic Net)
  - Decision trees and Random Forests
  - Gradient Boosting and XGBoost
  - Support Vector Machines
  - Neural Networks
- **Advanced diagnostics** for model assumptions and influence analysis
- **Interactive visualizations** for model understanding and comparison
- **Hyperparameter tuning** with grid and random search
- **Cross-validation** for robust model evaluation
- **Pipeline interface** for end-to-end modeling workflows
- **Integration** with other popular R packages like caret and tidymodels

## Installation

```r
# Install from CRAN
install.packages("tidylearn")

# Or install the development version from GitHub
# install.packages("devtools")
devtools::install_github("ces0491/tidylearn")
```

## Quick Start

```r
library(tidylearn)

# Load example data
data(mtcars)

# Train a simple linear model
model <- tl_model(mtcars, mpg ~ wt + hp + am, method = "linear")

# Make predictions
predictions <- predict(model, newdata = mtcars)

# Evaluate model performance
metrics <- tl_evaluate(model)
print(metrics)

# Plot diagnostics
plot(model, type = "residuals")

# Create an end-to-end modeling pipeline
pipeline <- tl_pipeline(
  data = mtcars,
  formula = mpg ~ .,
  models = list(
    linear = list(method = "linear"),
    lasso = list(method = "lasso"),
    forest = list(method = "forest", ntree = 500)
  )
)

# Run the pipeline
results <- tl_run_pipeline(pipeline)

# Compare model performance
tl_compare_pipeline_models(results)

# Get the best model
best_model <- tl_get_best_model(results)
```

## Main Functions

### Core Modeling

- `tl_model()` - Create a supervised learning model
- `predict()` - Make predictions with a model
- `tl_evaluate()` - Evaluate model performance

### Model Selection and Tuning

- `tl_step_selection()` - Perform stepwise feature selection
- `tl_tune_grid()` - Tune hyperparameters with grid search
- `tl_tune_random()` - Tune hyperparameters with random search
- `tl_tune_xgboost()` - Tune XGBoost models
- `tl_compare_cv()` - Compare models with cross-validation

### Diagnostics and Visualization

- `tl_check_assumptions()` - Check model assumptions
- `tl_influence_measures()` - Calculate influence metrics
- `tl_plot_importance()` - Plot feature importance
- `tl_plot_interaction()` - Visualize interaction effects
- `tl_diagnostic_dashboard()` - Create comprehensive diagnostic dashboard
- `tl_plot_xgboost_shap_summary()` - Plot SHAP values for model interpretation

### Pipeline Interface

- `tl_pipeline()` - Create an end-to-end modeling pipeline
- `tl_run_pipeline()` - Run a modeling pipeline
- `tl_get_best_model()` - Extract the best model from a pipeline
- `tl_compare_pipeline_models()` - Compare models from a pipeline

### Integration

- `tl_to_caret()` - Convert a tidylearn model to a caret model
- `tl_from_caret()` - Convert a caret model to a tidylearn model
- `tl_to_tidymodels()` - Convert a tidylearn model to a tidymodels workflow
- `tl_from_tidymodels()` - Convert a tidymodels workflow to a tidylearn model
- `tl_export_model()` - Export a model to various formats (RDS, ONNX, PMML, JSON)
- `tl_import_model()` - Import a model from various formats

## Examples

### Linear Regression with Diagnostics

```r
# Load data
data(mtcars)

# Create linear model
linear_model <- tl_model(mtcars, mpg ~ wt + hp + am, method = "linear")

# Check model assumptions
assumptions <- tl_check_assumptions(linear_model)
print(assumptions$overall$status)

# Plot diagnostic dashboard
tl_diagnostic_dashboard(linear_model)

# Calculate influence measures
influence <- tl_influence_measures(linear_model)
head(influence[influence$is_influential, ])

# Plot influential observations
tl_plot_influence(linear_model, plot_type = "leverage")
```

### Model Comparison with Cross-Validation

```r
# Load data
data(mtcars)

# Create models
linear_model <- tl_model(mtcars, mpg ~ wt + hp + am, method = "linear")
lasso_model <- tl_model(mtcars, mpg ~ ., method = "lasso")
forest_model <- tl_model(mtcars, mpg ~ ., method = "forest", ntree = 500)

# Compare with cross-validation
cv_results <- tl_compare_cv(
  data = mtcars,
  models = list(linear = linear_model, lasso = lasso_model, forest = forest_model),
  folds = 5,
  metrics = c("rmse", "mae", "rsq")
)

# Plot comparison
tl_plot_cv_comparison(cv_results)

# Statistically test differences
test_results <- tl_test_model_difference(
  cv_results,
  baseline_model = "linear",
  metric = "rmse"
)
print(test_results)
```

### Classification with XGBoost

```r
# Load data
data(iris)

# Train XGBoost model
xgb_model <- tl_model(
  iris, 
  Species ~ ., 
  method = "xgboost",
  nrounds = 100,
  max_depth = 3,
  eta = 0.1
)

# Evaluate model
metrics <- tl_evaluate(xgb_model, metrics = c("accuracy", "precision", "recall", "f1"))
print(metrics)

# Plot feature importance
tl_plot_xgboost_importance(xgb_model)

# Calculate and plot SHAP values
tl_plot_xgboost_shap_summary(xgb_model)

# Plot SHAP dependence for a specific feature
tl_plot_xgboost_shap_dependence(xgb_model, feature = "Petal.Width")
```

### End-to-End Pipeline with Multiple Models

```r
# Load data
data(mtcars)

# Create and run pipeline
pipeline <- tl_pipeline(
  data = mtcars,
  formula = mpg ~ .,
  preprocessing = list(
    impute_missing = TRUE,
    standardize = TRUE,
    dummy_encode = TRUE
  ),
  models = list(
    linear = list(method = "linear"),
    lasso = list(method = "lasso"),
    ridge = list(method = "ridge"),
    tree = list(method = "tree"),
    forest = list(method = "forest", ntree = 500)
  ),
  evaluation = list(
    metrics = c("rmse", "mae", "rsq"),
    validation = "cv",
    cv_folds = 5,
    best_metric = "rmse"
  )
)

results <- tl_run_pipeline(pipeline)

# Visualize model comparison
tl_compare_pipeline_models(results)

# Get best model
best_model <- tl_get_best_model(results)
summary(best_model)

# Make predictions with the pipeline
new_data <- mtcars[1:5, ]
predictions <- tl_predict_pipeline(results, new_data)
print(predictions)

# Save the pipeline
tl_save_pipeline(results, "my_pipeline.rds")
```

## Documentation

For detailed documentation, examples, and vignettes, please visit [https://ces0491.github.io/tidylearn/](https://ces0491.github.io/tidylearn/).

## Contributing

Contributions to tidylearn are welcome! Here's how you can contribute:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes
4. Run the tests (`devtools::test()`)
5. Commit your changes (`git commit -m 'Add some amazing feature'`)
6. Push to the branch (`git push origin feature/amazing-feature`)
7. Open a Pull Request

Please make sure your code follows the [tidyverse style guide](https://style.tidyverse.org/).

## Dependencies

tidylearn integrates with several excellent machine learning packages:

- **Core**: dplyr, tidyr, purrr, tibble, ggplot2, magrittr
- **Models**: glmnet, randomForest, rpart, gbm, e1071, nnet, keras/tensorflow
- **Evaluation**: yardstick, ROCR
- **Preprocessing**: recipes, rsample

## License

This package is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- The tidyverse team for their elegant approach to data science in R
- Authors of the various machine learning packages that tidylearn builds upon
